
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/tutorial_03_variogram_models.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorial_03_variogram_models.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorial_03_variogram_models.py:


3 - Variogram Models
====================

This tutorial will guide you through the theoretical variogram models available for the :class:`Variogram <skgstat.Variogram>` class. 

**In this tutorial you will learn:**

    * how to choose an appropiate model function
    * how to judge fitting quality
    * about sample size influence

.. GENERATED FROM PYTHON SOURCE LINES 14-21

.. code-block:: default

    import skgstat as skg
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import warnings
    warnings.filterwarnings('ignore')
    skg.plotting.backend('matplotlib')







.. GENERATED FROM PYTHON SOURCE LINES 22-29

3.1 Load data
-------------
For this example we will use the pancake dataset. You can use the
:mod:``skgstat.data`` submodule to directly sample the dataset. This is the
red-channel of an image of an actual pancake. The intersting thing about this pancake is,
that it shows some clear spatial structures in its browning, but of different 
shapes at different scales. This should be reflectable with different samples.

.. GENERATED FROM PYTHON SOURCE LINES 29-35

.. code-block:: default

    s = [30, 80, 300]
    data1 = skg.data.pancake(N=s[0], seed=42, as_dataframe=True).get('sample')
    data2 = skg.data.pancake(N=s[1], seed=42, as_dataframe=True).get('sample')
    data3 = skg.data.pancake(N=s[2], seed=42, as_dataframe=True).get('sample')









.. GENERATED FROM PYTHON SOURCE LINES 36-37

Plotting:

.. GENERATED FROM PYTHON SOURCE LINES 37-46

.. code-block:: default

    def plot_scatter(data, ax):
        art = ax.scatter(data.x, data.y, 50, c=data.v, cmap='plasma')
        plt.colorbar(art, ax=ax)

    # run
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    for data, ax in zip((data1, data2, data3), axes.flatten()):
        plot_scatter(data, ax)




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_001.png
   :alt: tutorial 03 variogram models
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 47-52

3.2 Comparing theoretical models
--------------------------------
One of the features of :mod:`skgstat` is the fact that it is programmed object oriented.
That means, we can just instantiate a :class:`Variogram <skgstat.Variogram>` object
and start changing arguments unitl it models spatial dependency in our observations well.

.. GENERATED FROM PYTHON SOURCE LINES 52-55

.. code-block:: default

    V1 = skg.Variogram(data1[['x', 'y']].values, data1.v.values, maxlag='median', normalize=False)
    V1.plot(show=False);




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_002.png
   :alt: tutorial 03 variogram models
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <Figure size 800x500 with 2 Axes>



.. GENERATED FROM PYTHON SOURCE LINES 56-57

Plot the others as well

.. GENERATED FROM PYTHON SOURCE LINES 57-76

.. code-block:: default

    V2 = skg.Variogram(data2[['x', 'y']].values, data2.v.values, maxlag='median', normalize=False)
    V3 = skg.Variogram(data3[['x', 'y']].values, data3.v.values, maxlag='median', normalize=False)

    fig, _a = plt.subplots(1, 3, figsize=(12, 3), sharey=True)
    axes = _a.flatten()

    x = np.linspace(0, V1.maxlag, 100)

    # plot each variogram
    for i, v in enumerate([V1, V2, V3]):
        axes[i].plot(v.bins, v.experimental, '.b')
        axes[i].plot(x, v.fitted_model(x), '-g')
        axes[i].set_title(f'N = {s[i]}')
        axes[i].set_xlabel('Lag (-)')
        if i == 0:
            axes[0].set_ylabel('semivariance (matheron)')
        axes[i].grid(which='major', axis='x')
    plt.tight_layout()




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_003.png
   :alt: N = 30, N = 80, N = 300
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 77-83

We can see how the experimental variogram changes dramatically with sample size.
Depending on the sample size, we can also choose different number of lag classes.
As the :class:`Variogram <skgstat.Variogram>`` is object oriented, we can simply
update the binning function. First we set the number of lags directly, then we derive
it from the distance matrix distribution. In the code below, we build the plot
from scratch, demonstrating how you can access the empirical data and how it is updated, when new parameters are supplied.

.. GENERATED FROM PYTHON SOURCE LINES 83-117

.. code-block:: default

    fig, axes = plt.subplots(3, 3, figsize=(12, 9), sharey=True, sharex=True)

    x = np.linspace(0, V1.maxlag, 100)
    manual_lags = (6, 12, 18)
    col_lab = ['10 lags', 'varying lags', 'Scott rule']

    # plot each variogram
    for i in range(3):
        for j, v in enumerate([V1, V2, V3]):
            # first row - use same settings
            if i == 0:
                v.bin_func = 'even'
                v.n_lags = 10
            # second row - use the manual lags
            if i == 1:
                v.n_lags = manual_lags[j]
            # last row - use scott
            if i == 2:
                v.bin_func = 'scott'
                axes[i][j].set_xlabel('Lag (-)')
        
            # plot
            axes[i][j].plot(v.bins, v.experimental, '.b')
            axes[i][j].plot(x, v.fitted_model(x), '-g')
            axes[i][j].grid(which='major', axis='x')
        
            # label first col
            if j == 0:
                axes[i][j].set_ylabel(col_lab[i])
            # title first row
            if i == 0:
                axes[i][j].set_title(f'N = {s[j]}')
    plt.tight_layout()




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_004.png
   :alt: N = 30, N = 80, N = 300
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 118-129

That actually demonstrates how the selection of the experimental variogram can 
have huge influence on the base data for fitting. Now consider the center column.
In each of the plots, the selection of model is not deterministic.
You can argue for at least two different models here, that might actually be supported by the empirical data.
The :class:`Variogram <skgstat.Variogram>` class has several goodness of fit
measures to help you on assessing the fit. This does not replace a careful
visual inspection of the models, but can assist you in making an decision.
Remember that the Kriging will be influenced by the quality of the spatial model,
especially on short distances.
We can quickly cycle all available models for the sample size of 80 to see
if spherical fits best. The histogram plot can be turned off.

.. GENERATED FROM PYTHON SOURCE LINES 129-140

.. code-block:: default


    # we use the settings from before - scott rule
    V2.bin_func = 'scott'
    fig, _a = plt.subplots(2,3, figsize=(12, 6), sharex=True, sharey=True)
    axes = _a.flatten()
    for i, model in enumerate(('spherical', 'exponential', 'gaussian', 'matern', 'stable', 'cubic')):
        V2.model = model
        V2.plot(axes=axes[i], hist=False, show=False)
        axes[i].set_title('Model: %s; RMSE: %.2f' % (model, V2.rmse))
        axes[i].set_ylim(0, 2000)




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_005.png
   :alt: Model: spherical; RMSE: 149.06, Model: exponential; RMSE: 150.89, Model: gaussian; RMSE: 150.48, Model: matern; RMSE: 144.37, Model: stable; RMSE: 144.95, Model: cubic; RMSE: 151.16
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 141-151

This is quite important. We find all 6 models to describe the experimental
variogram more or less equally well in terms of RMSE. Think of the
implications: We basically can use any model we like. 
This is a problem as i.e. the gaussian and the spherical model describe
fundamentally different spatial properties. Thus, our model selection
should be driven by interpretation of the variogram, and not the difference
in RMSE of only 0.4%, which might very likely not be significant at all.


But what does this difference look like, when it comes to interpolation?

.. GENERATED FROM PYTHON SOURCE LINES 151-161

.. code-block:: default


    def interpolate(V, ax):
        xx, yy = np.mgrid[0:499:100j, 0:499:100j]
        ok = skg.OrdinaryKriging(V, min_points=5, max_points=15, mode='exact')
        field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)
        art = ax.matshow(field, origin='lower', cmap='plasma', vmin=V.values.min(), vmax=V.values.max())
        ax.set_title('%s model' % V.model.__name__)
        plt.colorbar(art, ax=ax)
        return field








.. GENERATED FROM PYTHON SOURCE LINES 162-171

.. code-block:: default


    fields = []
    fig, _a = plt.subplots(2,3, figsize=(12, 10), sharex=True, sharey=True)
    axes = _a.flatten()
    for i, model in enumerate(('spherical', 'exponential', 'gaussian', 'matern', 'stable', 'cubic')):
        V2.model = model
        fields.append(interpolate(V2, axes[i]))





.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_006.png
   :alt: spherical model, exponential model, gaussian model, matern model, stable model, cubic model
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_006.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Warning: for 110 locations, not enough neighbors were found within the range.
    Warning: for 9 locations, not enough neighbors were found within the range.




.. GENERATED FROM PYTHON SOURCE LINES 172-173

Get some basic statistics about the fields

.. GENERATED FROM PYTHON SOURCE LINES 173-177

.. code-block:: default


    pd.DataFrame({'spherical': fields[0].flatten(), 'exponential': fields[1].flatten(), 'gaussian': fields[2].flatten(),
                  'matern': fields[3].flatten(), 'stable': fields[4].flatten(), 'cubic': fields[5].flatten()}).describe()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>spherical</th>
          <th>exponential</th>
          <th>gaussian</th>
          <th>matern</th>
          <th>stable</th>
          <th>cubic</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>10000.000000</td>
          <td>10000.000000</td>
          <td>9890.000000</td>
          <td>10000.000000</td>
          <td>9991.000000</td>
          <td>10000.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>184.534353</td>
          <td>184.088046</td>
          <td>186.461872</td>
          <td>184.448945</td>
          <td>184.338187</td>
          <td>184.113831</td>
        </tr>
        <tr>
          <th>std</th>
          <td>28.085141</td>
          <td>26.591814</td>
          <td>48.119147</td>
          <td>29.461643</td>
          <td>28.974844</td>
          <td>35.914252</td>
        </tr>
        <tr>
          <th>min</th>
          <td>84.118439</td>
          <td>84.326126</td>
          <td>60.090550</td>
          <td>82.964854</td>
          <td>83.216674</td>
          <td>82.848879</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>169.168862</td>
          <td>169.127068</td>
          <td>158.318778</td>
          <td>168.355530</td>
          <td>168.418710</td>
          <td>160.532301</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>187.594699</td>
          <td>188.633479</td>
          <td>188.561999</td>
          <td>187.495606</td>
          <td>187.525038</td>
          <td>185.865217</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>205.382495</td>
          <td>202.859567</td>
          <td>216.272628</td>
          <td>206.765854</td>
          <td>206.335497</td>
          <td>212.219655</td>
        </tr>
        <tr>
          <th>max</th>
          <td>244.731745</td>
          <td>243.522517</td>
          <td>398.260677</td>
          <td>249.272935</td>
          <td>247.948390</td>
          <td>263.676958</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 178-186

This should illustrate, how important the selection of model is, even if no observation uncertainties are propagated into the analysis.

  1. Gaussian model is far off, producing estimations far outside the observed value ranges
  2. All other models seem produce quite comparable mean values
  3. BUT: the standard deviation is quite different
  4. The median of the field can vary by more than 3 units, even if we took the Gaussian model out

You have to remind that we had quite some observations. The selection of model becomes even more arbitrary with smaller samples and more importantly: We have to consider more than one equally probable parameterization of each model when the experimental is more spreaded.

.. GENERATED FROM PYTHON SOURCE LINES 186-196

.. code-block:: default


    # Finally, we can calculate the difference between the kriging fields to inspect the spread of estimations spatially:
    #
    field_min = np.nanmin(np.stack(fields, axis=2), axis=2)
    field_max = np.nanmax(np.stack(fields, axis=2), axis=2)

    fig, ax = plt.subplots(1, 1, figsize=(7,7))
    m = ax.matshow(field_max - field_min, origin='lower', cmap='Reds')
    plt.colorbar(m)




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_007.png
   :alt: tutorial 03 variogram models
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_007.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7fdd7b744280>



.. GENERATED FROM PYTHON SOURCE LINES 197-200

The colorbar is spanning the entire value range. Thus, given the minor differences in the fitting of the models, we would have to reject just any estimation based on an automatic fit, which is considering some uncertainties in the selection of parameters, because the RMSE values were too close.

To use the result from above, we need to justfy the selection of model first and manually fit the model based on expert knowledge.

.. GENERATED FROM PYTHON SOURCE LINES 202-205

3.3 Using other sample sizes
----------------------------
Let's have a look at the sparse sample again

.. GENERATED FROM PYTHON SOURCE LINES 205-207

.. code-block:: default

    V1.plot(show=False);




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_008.png
   :alt: tutorial 03 variogram models
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_008.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <Figure size 800x500 with 2 Axes>



.. GENERATED FROM PYTHON SOURCE LINES 208-217

This is a nugget-effect variogram. Thus we have to reject any geostatistical 
analysis based on this sample. It just does not expose any spatial pattern that 
can be exploited.

What about the denser sample. Increasing the sample size should reject some 
of the models. Remind, that we are sampling at more short distances and thus,
the variogram will be governed by the short ranged patterns of the field, while
the other samples are more dependent on the medium and large range patterns, as
there were less short separating distances sampled.

.. GENERATED FROM PYTHON SOURCE LINES 217-228

.. code-block:: default


    # we use the settings from before - scott rule
    V3.bin_func = 'scott'
    fig, _a = plt.subplots(2,3, figsize=(12, 6), sharex=True, sharey=True)
    axes = _a.flatten()
    for i, model in enumerate(('spherical', 'exponential', 'gaussian', 'matern', 'stable', 'cubic')):
        V3.model = model
        V3.plot(axes=axes[i], hist=False, show=False)
        axes[i].set_title('Model: %s; RMSE: %.2f' % (model, V3.rmse))
        axes[i].set_ylim(0, 2000)




.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_009.png
   :alt: Model: spherical; RMSE: 77.02, Model: exponential; RMSE: 94.01, Model: gaussian; RMSE: 122.53, Model: matern; RMSE: 71.19, Model: stable; RMSE: 68.50, Model: cubic; RMSE: 124.22
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_009.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 229-232

We can now clearly reject the cubic, gaussian and exponential model. 
I personally would also reject the spherical model we used in the fist place,
as it is systematically underestimating the semi-variance on short distances. 

.. GENERATED FROM PYTHON SOURCE LINES 232-240

.. code-block:: default

    d_fields = []
    fig, _a = plt.subplots(2,3, figsize=(18, 12), sharex=True, sharey=True)
    axes = _a.flatten()
    for i, model in enumerate(('spherical', 'exponential', 'gaussian', 'matern', 'stable', 'cubic')):
        V3.model = model
        d_fields.append(interpolate(V3, axes[i]))





.. image-sg:: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_010.png
   :alt: spherical model, exponential model, gaussian model, matern model, stable model, cubic model
   :srcset: /auto_examples/images/sphx_glr_tutorial_03_variogram_models_010.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 241-242

Again some statistics

.. GENERATED FROM PYTHON SOURCE LINES 242-246

.. code-block:: default


    pd.DataFrame({'spherical': d_fields[0].flatten(), 'exponential': d_fields[1].flatten(), 'gaussian': d_fields[2].flatten(),
                  'matern': d_fields[3].flatten(), 'stable': d_fields[4].flatten(), 'cubic': d_fields[5].flatten()}).describe()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>spherical</th>
          <th>exponential</th>
          <th>gaussian</th>
          <th>matern</th>
          <th>stable</th>
          <th>cubic</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>10000.000000</td>
          <td>10000.000000</td>
          <td>10000.000000</td>
          <td>10000.000000</td>
          <td>10000.000000</td>
          <td>10000.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>186.746369</td>
          <td>186.501460</td>
          <td>188.017572</td>
          <td>186.839652</td>
          <td>186.795057</td>
          <td>188.097637</td>
        </tr>
        <tr>
          <th>std</th>
          <td>32.576157</td>
          <td>32.353686</td>
          <td>100.519330</td>
          <td>33.397966</td>
          <td>33.084211</td>
          <td>35.940467</td>
        </tr>
        <tr>
          <th>min</th>
          <td>88.030671</td>
          <td>88.342581</td>
          <td>-1201.529481</td>
          <td>83.294112</td>
          <td>86.154432</td>
          <td>56.361098</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>167.688495</td>
          <td>166.802809</td>
          <td>156.078978</td>
          <td>167.855892</td>
          <td>167.771257</td>
          <td>168.910398</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>191.218375</td>
          <td>191.155730</td>
          <td>193.865877</td>
          <td>191.499715</td>
          <td>191.328717</td>
          <td>193.001095</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>211.672155</td>
          <td>211.317278</td>
          <td>222.579927</td>
          <td>212.025631</td>
          <td>211.922855</td>
          <td>215.165573</td>
        </tr>
        <tr>
          <th>max</th>
          <td>243.613737</td>
          <td>243.598604</td>
          <td>1178.639922</td>
          <td>244.335039</td>
          <td>243.948692</td>
          <td>265.643546</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 247-250

Finally, if we only concentrate on the not-rejected models: matern and stable,
we can see hardly any difference in the field. Additionally, except for extrema,
the statistical properties of the two fields are largely the same.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  1.433 seconds)


.. _sphx_glr_download_auto_examples_tutorial_03_variogram_models.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tutorial_03_variogram_models.py <tutorial_03_variogram_models.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tutorial_03_variogram_models.ipynb <tutorial_03_variogram_models.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
